{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layers\n",
    "\n",
    "We sample here a dataset where a linear classifier is not able to model the data. We will see in this notebook how the addition of hidden layers makes it possible to improve classification performance. We also use TF-Slim, a lightweight library for defining, training, and evaluating models in TensorFlow.\n",
    "\n",
    "With TF-Slim's higher level APIs we can reduce the number of lines of code to train the model. You will notice some overhead as our dataset needs to conform to the `slim.dataset.Dataset` API. It is worth going over this code, as you can use a similar approach when you need to define a dataset using your own data. \n",
    "\n",
    "You can visualize the loss using the TensorBoard web app. To do so, launch `tensorboard --logdir=solutions/train`. Note that you will have to periodically restart the tensorboard in order to visualize your latest training runs. \n",
    "\n",
    "Remember the master formula:\n",
    "$$ model = data + structure + loss + optimizer$$\n",
    "\n",
    "In this notebook, you will:\n",
    "- define a neural network with hidden layers.\n",
    "- train your network on the moon dataset.\n",
    "- improve your accuracy by tuning the network structure.\n",
    "\n",
    "You will use the slim API for all of these and here are some functions that will be helpful:\n",
    "\n",
    "```tf.nn.relu, tf.nn.tanh, slim.fully_connected, slim.losses.softmax_cross_entropy, tf.one_hot```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "## 1. Data\n",
    "We first start by generating some data that is more complicated than in the prior notebook. \n",
    "\n",
    "### Exercise 0\n",
    "- generate the moon dataset by executing the next block.\n",
    "\n",
    "- why will the prior model fail to train a classifier for this data? Discuss with your neighbor. (plug in this data into notebook 0 to see what happens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "X, labels = make_moons(n_samples=1000, noise=0.275, random_state=0)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, labels, test_size=.1, random_state=42)\n",
    "\n",
    "x_min = -2\n",
    "x_max = 3\n",
    "y_min = -1.5\n",
    "y_max = 2\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=20, linewidths=0)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.axes().set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save TFRecords dataset for training\n",
    "\n",
    "We save our dataset numpy arrays to a file in TFRecord format. TFRecords are a binary format that simplifies many data loading/processing tasks in Tensorflow, but is not strictly necessary to using Tensorflow. It is integrated well with the tf.slim API, so we will use it here. You can learn more about data formats for Tensorflow here: [reading_data](https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset utility functions\n",
    "\n",
    "_FILE_PATTERN = 'moons_%s.tfrecord'\n",
    "\n",
    "_SPLITS_TO_SIZES = {\n",
    "    'train': 900,\n",
    "    'validation': 100,\n",
    "}\n",
    "\n",
    "_NUM_CLASSES = 2\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'x': 'feature vector',\n",
    "    'y': 'binary label',\n",
    "}\n",
    "\n",
    "\n",
    "def _add_to_tfrecord(X, labels, tfrecord_writer):\n",
    "    \"\"\"adds data to a TFRecord\n",
    "    \"\"\"    \n",
    "    with tf.Session('') as sess:\n",
    "        for x, label in zip(X, labels):\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=x)),\n",
    "                'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "            }))\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "            \n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    if split_name not in _SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "    file_pattern= os.path.join(dataset_dir, file_pattern % split_name)\n",
    "    \n",
    "    # we store the dataset in TF Records\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader    \n",
    "\n",
    "    # decoder\n",
    "    keys_to_features = {\n",
    "        'x': tf.FixedLenFeature([2], dtype=tf.float32),\n",
    "        'y': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    }\n",
    "    items_to_handlers = {\n",
    "        'x': slim.tfexample_decoder.Tensor('x'),\n",
    "        'y': slim.tfexample_decoder.Tensor('y'),\n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=tf.TFRecordReader,\n",
    "        decoder=decoder,\n",
    "        num_samples=_SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        labels_to_names=None\n",
    "    )\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size=32):\n",
    "    \"\"\"Load data batch\n",
    "    \"\"\"\n",
    "    provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, shuffle=False)\n",
    "    x, y = provider.get(['x', 'y'])\n",
    "    return tf.train.batch(\n",
    "        [x, y],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=1,\n",
    "        capacity=100\n",
    "    )\n",
    "\n",
    "\n",
    "# write dataset in TFRecords format\n",
    "train_filename = _FILE_PATTERN % \"train\"\n",
    "with tf.python_io.TFRecordWriter(train_filename) as tfrecord_writer:\n",
    "    _add_to_tfrecord(X_train, y_train, tfrecord_writer)\n",
    "test_filename = _FILE_PATTERN % \"validation\"\n",
    "with tf.python_io.TFRecordWriter(test_filename) as tfrecord_writer:\n",
    "    _add_to_tfrecord(X_validation, y_validation, tfrecord_writer)\n",
    "\n",
    "    \n",
    "# iterate over a few batches\n",
    "dataset = get_split(\"train\", \".\")\n",
    "xb, yb = load_batch(dataset, batch_size=4)\n",
    "with tf.Session() as sess:\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        for i in range(2):\n",
    "            print 'batch %d'%i\n",
    "            x_np, y_np = sess.run([xb, yb])\n",
    "            print x_np, y_np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## 2. Structure\n",
    "\n",
    "You just learned about hidden layers. You can now define a model with hidden units. You can edit the function below to specify your network. The slim API lets you easily define new layers with the `slim.fully_connected` function.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "- Check out the function doc string for `slim.fully_connected` to learn how to use this function.\n",
    "- `slim.fully_connected` is a faster way to create a fully connected layer. In the previous notebook, we made a fully connected layer \"manually\". What code from the last notebook does `slim.fully_connected` replace?\n",
    "- Define the function body of `build_model(x)` below and create a network with 1 hidden layer and an output of dimension 2.\n",
    "\n",
    "Hint:\n",
    "- You will want to create the final layer in your network to be **linear**, i.e. not have an activation function. (Sometimes these output values are called \"logits\" if they are used as input to a sigmoid or softmax function.) This will simplify applying the loss function and is common practice for more modulare Tensorflow code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can get more info inside the notebook on all functions by typing in the function with a '?'\n",
    "slim.fully_connected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(x):\n",
    "    \"\"\"\n",
    "    Build a neural network. This function gets as the data x as an argument \n",
    "    and returns the last layer. The final layer will have a 2 dimensional output!\n",
    "    \"\"\"\n",
    "    # TODO define a deep model\n",
    "    \n",
    "    # e.g. fc1 = slim.fully_connected(inputs=x, num_outputs=2, activation_fn=tf.nn.relu, scope='fc1')\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## 3. Loss\n",
    "In the prior notebook, you computed the loss yourself. With the slim API\n",
    "\n",
    "### Exercise 2\n",
    "- Write down the loss function. The neural net has a 2 dimensional output. The labels for our dataset are 1-dimensional and either 0 or 1. How do you have to encode the network output to define your loss function?\n",
    "\n",
    "- Fill in the `TODO`s below. use `slim.losses.softmax_cross_entropy` for your loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir = \"train/%s\" % datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "# hyperparameters\n",
    "number_of_steps = 1000\n",
    "batch_size = 100\n",
    "learning_rate = .25\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # turn on a very verbose logging.\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "    # load training data\n",
    "    dataset = get_split(\"train\", \".\")\n",
    "    x, y = load_batch(dataset, batch_size=batch_size)\n",
    "\n",
    "    # define model\n",
    "    logits = build_model(x)\n",
    "\n",
    "    # TODO - define loss\n",
    "    loss = ...\n",
    "    \n",
    "    # tell tensorflow to log the loss value for visualization\n",
    "    tf.scalar_summary(\"loss\", loss)\n",
    "    \n",
    "    # summary op\n",
    "    summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## 4. Optimizer\n",
    "The last missing piece for our model is the optimizer. We will first use [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) to train our model. \n",
    "\n",
    "### Exercise 3\n",
    "- Create your optimizer using the `tf.train.GradientDescentOptimizer`\n",
    "- Define the train step with `slim.learning.create_train_op`\n",
    "\n",
    "Now your model is fully defined and you can\n",
    "- run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "\n",
    "    # TODO - define optimizer\n",
    "    optimizer = ...\n",
    "    \n",
    "    # TODO - define train operation\n",
    "    train_op = ...\n",
    "\n",
    "    # train the model\n",
    "    slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        log_every_n_steps=number_of_steps / 10,\n",
    "        number_of_steps=number_of_steps,\n",
    "        summary_op=summary_op,\n",
    "        save_summaries_secs=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect training\n",
    "You can now inspect the training procedure using `tensorboard`. It's a great tool to get insights into the setup of your network and its training behavior. launch it using\n",
    "```\n",
    "$ tensorboard --logdir='path/to/traindir'\n",
    "```\n",
    "and then navigate to `localhost:6006` and take a peak around.\n",
    "\n",
    "### Exercise 4\n",
    "Make yourself familiar with the tensorboard and try to answer the following questions.\n",
    "- Take a look at the `Scalars` tab and click on the loss line. What should things look like here?\n",
    "- Inspect the `Graphs` tab. Is this what you expected to see? \n",
    "- What do the number on the lines between the layers mean?\n",
    "- Click on the `+` in the top right corner of one of your `fully connected` layers. How many trainable parameters does this layer have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "You trained your network to minimize the loss. To make this result interpretable, we now compute the accuracy of the prediction.\n",
    "\n",
    "### Exercise 5\n",
    "- How do you predict the label from the computed output?\n",
    "- Fill in the missing line below and evaluate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # load validation data\n",
    "    dataset = get_split(\"validation\", \".\")\n",
    "    x, y = load_batch(dataset, batch_size=dataset.num_samples)\n",
    "    \n",
    "    # build model\n",
    "    logits = build_model(x)\n",
    "    \n",
    "    # TODO - get predictions from model (hint: tf.argmax)\n",
    "    predictions = ...\n",
    "    \n",
    "    # compute accuracy\n",
    "    acc_value_op, acc_update_op = slim.metrics.streaming_accuracy(predictions, y)\n",
    "        \n",
    "    # path to model checkpoint\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    print \"Model checkpoint:\", checkpoint_path\n",
    "    \n",
    "    # compute metrics\n",
    "    accuracy = slim.evaluation.evaluate_once(\n",
    "        master='',\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logdir=train_dir,\n",
    "        num_evals=1,\n",
    "        eval_op=acc_update_op,\n",
    "        final_op=acc_value_op\n",
    "    )\n",
    "    \n",
    "    print \"Accuracy =\", accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "Just like in the last notebook, we now want to look at these results.\n",
    "\n",
    "### Exercise 6\n",
    "- Visualize the training results. What should the contours look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    # build model and retrieve estimated probabilities\n",
    "    x = tf.placeholder(tf.float32, [None, 2], name='x-input')\n",
    "    logits = build_model(x)\n",
    "    probabilities = slim.softmax(logits)\n",
    "\n",
    "    # path to model checkpoint\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    print \"Model checkpoint:\", checkpoint_path\n",
    "\n",
    "    # grid points to draw decision boundary\n",
    "    h = 0.02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    x_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # restore variables from checkpoint\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "\n",
    "        # compute probabilitiese\n",
    "        probabilities_grid = sess.run(probabilities, feed_dict={x: x_grid})\n",
    "    \n",
    "\n",
    "# plot probabilities\n",
    "probabilities_grid = probabilities_grid[:, 1].reshape(xx.shape)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.contourf(xx, yy, probabilities_grid, cmap=plt.cm.RdBu_r, alpha=.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=20, linewidths=0)\n",
    "#plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=20, linewidths=0)\n",
    "#plt.scatter(X_validation[:, 0], X_validation[:, 1], c=y_validation, s=20, linewidths=0)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise 7\n",
    "\n",
    "Not bad so far. But you can do better! You should be able to achieve an accuracy of 93%\n",
    "\n",
    "Modify your model in the `build_graph` function:\n",
    "\n",
    "- try more layers.\n",
    "- try different numbers of hidden units.\n",
    "- what do you observe?\n",
    "- you should reach an accuracy of 90% on this task.\n",
    "- how good can you get overall?\n",
    "- why not better?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
