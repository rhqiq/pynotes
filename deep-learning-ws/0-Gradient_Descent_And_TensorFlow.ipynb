{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Gradient Descent Optimization\n",
    "\n",
    "In this notebook you will implement a simple machine learning algorithm yourself and then see how the same model can be implemented in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We use scikit-learn to create a simple artificial dataset that consists of two clusters. Our goal in this toy problem is to learn to distinguish \"red\" points from \"blue\" points. (By the way, scikit-learn has many toy datasets built-in and they are great for building intuition about how different methods work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a small \"blobs\" dataset\n",
    "X, y = make_blobs(n_samples=1000, n_features=2, cluster_std=1.0, centers=[(-2, 0), (2, 0)], random_state=42)\n",
    "\n",
    "print 'Vectors: \\n', X[:10]\n",
    "print 'Labels: \\n', y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of 1000 points each with 2 dimensions. Each point has an associated label (0 vs. 1) indicating which blob that point belongs to. Since we have labels for what we are trying to predict from the data (which blob the point belongs to), this is called a **supervised** problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the data points\n",
    "x_min = y_min = -5\n",
    "x_max = y_max = 5\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, linewidths=0)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpecting the dataset, we can see that this is an \"easy\" dataset because the points on are only 2 dimensions and the blobs are, roughly speaking, **linearly separable** - this means that we can do well on the problem by finding a line that divides the two blobs and making a decision by simply finding what side of the line each point is on.\n",
    "\n",
    "Next we will split our data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear classifier with Python\n",
    "\n",
    "We use a logistic regression classifier, which we optimize using stochastic gradient descent. We provide below helper functions to compute the estimated probability, loss, gradients, and classifier accuracy. \n",
    "\n",
    "### Exercise 0 - define model functions\n",
    "\n",
    "We need to make some basic decisions about the structure of our model. We will first do this in python before moving our model to Tensorflow. Below are four functions we need to define along with tests to let you know if you have it correct. Using the mathematical definitions of those functions below, fill in the correct python code for each function.\n",
    "\n",
    "**1) Probability**\n",
    "\n",
    "First we need to define how our model computes $p(y=1|x)$, the probability of a point belonging to blob 1 (the probability of belong to blob 0 is just $1-p(y=1|x)$. In our model, this is defined as $p(y=1|x) = \\sigma(xW + b)$ where $\\sigma$ is the **logistic sigmoid** activation function, $\\sigma(t) = \\frac{1}{1 + e^{-t}}$.\n",
    "\n",
    "Fill in `def prob(X, w, b)` below to implement this function given a matrix of examples `X`, a matrix of weights `w`, and a vector of biases `b`.\n",
    "\n",
    "Hint: you will need to use the `np.dot` function for weight matrix multiplication and `np.exp` for the exponential.\n",
    "\n",
    "**2) Prediction**\n",
    "\n",
    "Once we have a probability, we need a function to turn it into a decision. In this case, we should return the integer label we predict for each example, which is simply $1$ if $p(y=1|x) > 0.5$ and $0$ otherwise.\n",
    "\n",
    "Complete `def predict(X, w, b)` to return a decision for each example.\n",
    "\n",
    "**3) Loss**\n",
    "\n",
    "In order to learn, we must define a **loss** function that gives us a score telling us how well our model is doing. For this problem, we will use **sigmoid cross-entropy** loss which is defined as $L =-y \\log(p) - (1-y)\\log(1 - p)$ where $y$ is the label for an example (0 or 1) and $p$ is the probability our model assigns to the example.\n",
    "\n",
    "You can learn more about cross-entropy loss [here](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression), though you won't need this to complete the exercise.\n",
    "\n",
    "**4) Parameter update**\n",
    "\n",
    "Now that we have the loss, we can compute the **gradients** of the loss with respect to our parameters which tells us how to change the parameters to improve performance. We have two different sets of parameters: the weights and biases. The gradient of the loss function with respect to each of these is, repectively, $\\frac{\\partial L}{\\partial w} = -(y-p)x$ and $\\frac{\\partial L}{\\partial b} = -(y-p)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def prob(X, w, b):\n",
    "    \"\"\"\n",
    "    Compute posterior probability p(y=1|x)\n",
    "    \"\"\"\n",
    "    # TODO - implement probability function\n",
    "    return ...\n",
    "    \n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Predict labels\n",
    "    \"\"\"\n",
    "    p = prob(X, w, b)\n",
    "    \n",
    "    # TODO - implement prediction function\n",
    "    return ...\n",
    "\n",
    "def loss(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute loss function\n",
    "    \"\"\"\n",
    "    # compute posterior probability\n",
    "    p = prob(X, w, b)\n",
    "\n",
    "    # TODO - compute cross-entropy loss\n",
    "    l = ...\n",
    "    \n",
    "    return l.mean()\n",
    "\n",
    "def grad_loss(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradient of the loss\n",
    "    \"\"\"\n",
    "    p = prob(x, w, b)\n",
    "\n",
    "    # TODO - compute gradients for weights and biases\n",
    "    dLdw = ...\n",
    "    dLdb = ...\n",
    "\n",
    "    return dLdw, dLdb\n",
    "\n",
    "\n",
    "\n",
    "# generate test data\n",
    "np.random.seed(42)\n",
    "rX = np.random.randn(1, 2)\n",
    "ry = 0\n",
    "rw = np.random.randn(2, 1)\n",
    "rb = np.random.randn(1)\n",
    "\n",
    "print 'Test passing for \"prob\": ', np.allclose(prob(rX, rw, rb), 0.46928423)\n",
    "print 'Test passing for \"predict\": ', np.allclose(predict(rX, rw, rb), 0)\n",
    "print 'Test passing for \"loss\": ', np.allclose(loss(rX, ry, rw, rb), 0.63352868)\n",
    "gw, gb = grad_loss(rX, ry, rw, rb)\n",
    "print 'Test passing for \"grad_loss\": ', np.allclose(gw, np.array([[ 0.23310012, -0.06488526]])) and \\\n",
    "                                        np.allclose(gb, 0.46928423)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - implement the parameter update in the training loop\n",
    "\n",
    "Below is the training loop for the logistic regression classifier we defined above. Before running the code, you need to implement one last detail: the paramter update itself. To do **gradient descent**, we need to take the gradient for the parameters, multiple that gradient by a **learning rate** parameter (below called `eta`) to get the update, and modify the parameters with that update. The different between mimimizing the loss function and maximizing the loss function is the difference between subtracting the update and adding the update.\n",
    "\n",
    "Fill in the update equations below and train the model. How well does your model do on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize classifier parameters\n",
    "w = np.random.random((2,))\n",
    "b = np.random.random((1,))\n",
    "\n",
    "# learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# number of iterations\n",
    "num_iterations = 500\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # select training sample\n",
    "    idx = i % X_train.shape[0]\n",
    "    \n",
    "    # compute gradients with respect to parameters for this example\n",
    "    dw, db = grad_loss(X_train[idx], y_train[idx], w, b)\n",
    "    \n",
    "    # TODO - write update rule for w and b\n",
    "    ...\n",
    "    \n",
    "    # print out performance every 20 iterations\n",
    "    if i % (num_iterations / 20) == 0:\n",
    "        y_train_hat = predict(X_train, w, b)\n",
    "        y_test_hat = predict(X_test, w, b)\n",
    "        print \"iter: %04d: loss(train) = %.3f, acc(train) = %.3f, loss(test)= %.3f, acc(test) = %.3f\" % \\\n",
    "            (i, loss(X_train, y_train, w, b), (y_train == y_train_hat).mean(), \n",
    "             loss(X_test, y_test, w, b), (y_test == y_test_hat).mean())\n",
    "\n",
    "\n",
    "# visualize the decision boundary\n",
    "h = 0.02\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "G = np.c_[xx.ravel(), yy.ravel()]\n",
    "G_prob = prob(G, w, b)\n",
    "G_prob = G_prob.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.contourf(xx, yy, G_prob, cmap=plt.cm.RdBu_r, alpha=.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, linewidths=0)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression classifier in TensorFlow\n",
    "\n",
    "Here we will train the same classifier as above, but we will be using TensorFlow and make use of automatic differentiation.\n",
    "\n",
    "Tensorflow (and other deep learning frameworks like it) takes away a lot of the manual effort in defining losses, update formulas, etc. This frees us to focus more on the modeling and less on the mechanics.\n",
    "\n",
    "### Exercise 2 - Tensorflow model parts\n",
    "\n",
    "In the block below, we define all the pieces of our model. Find the following components:\n",
    "1. The `tf.placeholder` defines the inputs to our model. In this case, we have two. What are they?\n",
    "2. The `tf.Variable` defines the learnable parameters of our model. What are they? How are they initialized here?\n",
    "\n",
    "\n",
    "### Exercise 3 - Define the model\n",
    "\n",
    "Below you will fill in the same components as you did above, but you will use Tensorflow.\n",
    "\n",
    "1. First define the model. This should be a single line that uses `tf.matmul` to multiply the input data `x_` with the weights `w_` then add the bias `b_`. Assign this to the `z_` variable.\n",
    "2. Next define the prediction function. It should take `z_` as input and compute the probability using the sigmoid function. You will need to use `tf.exp`, but the rest of the operations can be written like in normal python.\n",
    "3. Finally, you can use Tensorflow's built-in [\"sigmoid cross entropy\" loss function](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits). This function takes `z_` as input (called the \"logits\") and the target output, `y_`, and returns the loss.\n",
    "\n",
    "After doing the above, you are ready to train the model with Tensorflow! Notice that you didn't have to define the gradient of the loss. This is one of the things that Tensorflow can figure out for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders define the inputs to our model\n",
    "x_ = tf.placeholder(tf.float32, [None, 2])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# variables define the parameters of out model\n",
    "w_ = tf.Variable(tf.zeros([2, 1]))\n",
    "b_ = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "# TODO - define the model\n",
    "z_ = ...\n",
    "\n",
    "# TODO - define the prediction function\n",
    "p_ = ...\n",
    "\n",
    "# TODO - define the loss function\n",
    "loss_ = ...\n",
    "\n",
    "\n",
    "loss_ = tf.reduce_mean(loss_)\n",
    "\n",
    "# compute accuracy\n",
    "y_hat_ = tf.cast(p_ > .5, \"float\")\n",
    "correct_prediction_ = tf.equal(y_hat_, y_)\n",
    "accuracy_ = tf.reduce_mean(tf.cast(correct_prediction_, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model you defined above by running the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "num_iterations = 100\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss_)\n",
    "# train_step = tf.train.AdamOptimizer(0.1).minimize(loss_)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # loop over the training data\n",
    "    for i in range(num_iterations):\n",
    "        idx = i % X_train.shape[0]\n",
    "        _, l = sess.run([train_step, loss_], \n",
    "                        feed_dict={x_: X_train[idx].reshape(1, X.shape[1]), \n",
    "                                   y_: y_train[idx].reshape(1, 1)})\n",
    "        \n",
    "        # print out performance every 20 iterations\n",
    "        if i % (num_iterations / 20) == 0:\n",
    "            loss_train, accuracy_train = sess.run([loss_, accuracy_],\n",
    "                                                  feed_dict={x_: X_train,\n",
    "                                                             y_: y_train.reshape((len(y_train), 1))})\n",
    "            loss_test, accuracy_test = sess.run([loss_, accuracy_],\n",
    "                                                feed_dict={x_: X_test,\n",
    "                                                           y_: y_test.reshape((len(y_test), 1))})            \n",
    "            print \"it: %04d: loss(train) = %.3f, acc(train) = %.3f, loss(test)= %.3f, acc(test) = %.3f\" % \\\n",
    "                (i, loss_train, accuracy_train, loss_test, accuracy_test)\n",
    "    \n",
    "    # compute estimated probabilities on the grid for visualization\n",
    "    G_prob = sess.run(p_, feed_dict={x_: G})\n",
    "    G_prob = G_prob.reshape(xx.shape)\n",
    "\n",
    "# plot decision boundary\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.contourf(xx, yy, G_prob, cmap=plt.cm.RdBu_r, alpha=.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, linewidths=0)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
