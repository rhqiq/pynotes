{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning MNIST\n",
    "\n",
    "In this exercise you will design a classifier for the very simple but very popular [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a classic of dataset in computer vision and one of the first real world problems solved by neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset in TFRecords Format\n",
    "\n",
    "The dataset is available on [Gdrive](https://drive.google.com/drive/folders/0B7DvVTpUhOA9S0Ryek11Sk1oNTA?usp=sharing). You should download the files and put them in the `DATA_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some utilities to load the dataset. Make sure you have placed the dataset in the `DATA_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MNIST dataset utilities\n",
    "\n",
    "DATA_DIR = 'mnist/'\n",
    "\n",
    "_FILE_PATTERN = 'mnist_%s.tfrecord'\n",
    "\n",
    "_SPLITS_TO_SIZES = {'train': 60000, 'test': 10000}\n",
    "\n",
    "_NUM_CLASSES = 10\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'image': 'A [28 x 28 x 1] grayscale image.',\n",
    "    'label': 'A single integer between 0 and 9',\n",
    "}\n",
    "\n",
    "LABELS_FILENAME = 'labels.txt'\n",
    "\n",
    "\n",
    "def read_label_file(dataset_dir, filename=LABELS_FILENAME):\n",
    "    \"\"\"Reads the labels file and returns a mapping from ID to class name.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: The directory in which the labels file is found.\n",
    "        filename: The filename where the class names are written.\n",
    "\n",
    "    Returns:\n",
    "        A map from a label (integer) to class name.\n",
    "    \"\"\"\n",
    "    labels_filename = os.path.join(dataset_dir, filename)\n",
    "    with tf.gfile.Open(labels_filename, 'r') as f:\n",
    "        lines = f.read().decode()\n",
    "    lines = lines.split('\\n')\n",
    "    lines = filter(None, lines)\n",
    "\n",
    "    labels_to_class_names = {}\n",
    "    for line in lines:\n",
    "        index = line.index(':')\n",
    "        labels_to_class_names[int(line[:index])] = line[index+1:]\n",
    "    return labels_to_class_names\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading MNIST.\n",
    "\n",
    "    Args:\n",
    "        split_name: A train/test split name.\n",
    "        dataset_dir: The base directory of the dataset sources.\n",
    "        file_pattern: The file pattern to use when matching the dataset sources.\n",
    "          It is assumed that the pattern contains a '%s' string so that the split\n",
    "          name can be inserted.\n",
    "        reader: The TensorFlow reader type.\n",
    "\n",
    "    Returns:\n",
    "        A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if `split_name` is not a valid train/test split.\n",
    "  \"\"\"\n",
    "    if split_name not in _SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\n",
    "        'image/class/label': tf.FixedLenFeature(\n",
    "            [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label', shape=[]),\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    labels_to_names = read_label_file(dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=reader,\n",
    "        decoder=decoder,\n",
    "        num_samples=_SPLITS_TO_SIZES[split_name],\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        labels_to_names=labels_to_names)\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size=32):\n",
    "    \"\"\"Load a single batch of data\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32, common_queue_min=8)\n",
    "    \n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    image = tf.to_float(image)\n",
    "    image = (1. / 255) * image\n",
    "    \n",
    "    # batch it up\n",
    "    images, labels = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=1,\n",
    "        capacity=2 * batch_size\n",
    "    )\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to view some examples to build up an intuition about your dataset. The block below creates a `DatasetDataProvider` object that can read data examples from the TFRecord files. It then plots some of the images in the dataset along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display a few examples\n",
    "dataset = get_split(\"train\", DATA_DIR)\n",
    "provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "    dataset, shuffle=False)\n",
    "image, label = provider.get([\"image\", \"label\"])\n",
    "with tf.Session() as sess:\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        for _ in range(5):\n",
    "            np_image, np_label = sess.run([image, label])\n",
    "            height, width, _  = np_image.shape\n",
    "            class_name = name = dataset.labels_to_names[np_label]\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.imshow(np_image.squeeze(), cmap=plt.cm.gray, interpolation='None')\n",
    "            plt.title('%s, %d x %d' % (name, height, width))\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - design an MLP for MNIST\n",
    "\n",
    "Build an MLP. It is up to you what the structure of the model will be, but keep in mind that this problem is much higher dimensional than previous problems we have worked on. This is your first chance to design a model on real data! See if you can get 90% accuracy or better.\n",
    "\n",
    "Here are some of the things you will need to decide about your model:\n",
    "- number of layers\n",
    "- activation function\n",
    "- number of dimensions in each layer\n",
    "- batch size\n",
    "- number of epochs\n",
    "- learning rate\n",
    "\n",
    "Suggestions:\n",
    "- We will treat each images as one large vector. To do this, the first thing you need to do is \"flatten\" the images before feeding them into more layers. Check out `slim.flatten` to do this.\n",
    "- The training logs are saved in `train_mnist/`, so you can use Tensorboard to help you visualize training.\n",
    "- Feel free to compare results with you  neighbors to find out what works well.\n",
    "- You may be able to improve your test performance by regularizing you model. Check out `slim.dropout`.\n",
    "- You may want to try a more sophisticated optimizer. Tensorflow's `AdamOptimizer` is a good choice that takes care of tuning the learning rate for you.\n",
    "\n",
    "\n",
    "If you want to talk over design decisions, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define your model\n",
    "def my_model(images, num_classes):\n",
    "    \"\"\"Build a basic neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO - design a model for MNIST\n",
    "    \n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "This block will train the model that you define above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir = \"train_mnist/%s\" % datetime.now().strftime(\"%H-%M-%S\")\n",
    "batch_size = 100\n",
    "learning_rate = 0.5\n",
    "num_epochs = 2\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    \n",
    "    dataset = get_split('train', DATA_DIR)\n",
    "    images, labels = load_batch(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # create the model\n",
    "    logits = my_model(images, dataset.num_classes)\n",
    "    \n",
    "    # specify the loss function\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    \n",
    "    # create some summaries to visualize the training process\n",
    "    tf.scalar_summary('losses/Total Loss', total_loss)\n",
    "    \n",
    "    # specify the otpimizer and create the train op\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#     optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # run the training\n",
    "    slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        log_every_n_steps=50,\n",
    "        number_of_steps=dataset.num_samples * num_epochs / batch_size,\n",
    "        save_summaries_secs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance\n",
    "\n",
    "Run the block below to evaluate your latest model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    \n",
    "    dataset = get_split('test', DATA_DIR)\n",
    "    images, labels = load_batch(dataset, batch_size=dataset.num_samples)\n",
    "    \n",
    "    # build model and retrieve predictions\n",
    "    logits = my_model(images, dataset.num_classes)\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    \n",
    "    # accuracy metric\n",
    "    acc_value_op, acc_update_op = slim.metrics.streaming_accuracy(predictions, labels)\n",
    "    \n",
    "    # model checkpoint\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    print \"Model checkpoint:\", checkpoint_path\n",
    "    \n",
    "    accuracy = slim.evaluation.evaluate_once(\n",
    "        master='',\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logdir=train_dir,\n",
    "        num_evals=1,\n",
    "        eval_op=acc_update_op,\n",
    "        final_op=acc_value_op\n",
    "    )\n",
    "    \n",
    "    print \"Accuracy:\", accuracy\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
