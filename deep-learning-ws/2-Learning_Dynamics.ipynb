{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Dynamics\n",
    "\n",
    "We return again to the \"half moons\" dataset you explored in the last notebook to understand how the dynamics of the learning process changes as you change key hyperparameters. In particular, we will look at the connection between some important hyperparameters and how well or quickly the model learns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions that load the dataset we have previously saved as TFRecord files, as well as a function to define a simple MLP that you will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset utility functions\n",
    "\n",
    "_SPLITS_TO_SIZES = {\n",
    "    'train': 900,\n",
    "    'validation': 100,\n",
    "}\n",
    "\n",
    "_NUM_CLASSES = 2\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'x': 'feature vector',\n",
    "    'y': 'binary label',\n",
    "}\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    if split_name not in _SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "    file_pattern= os.path.join(dataset_dir, file_pattern % split_name)\n",
    "    \n",
    "    # we store the dataset in TF Records\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader    \n",
    "\n",
    "    # decoder\n",
    "    keys_to_features = {\n",
    "        'x': tf.FixedLenFeature([2], dtype=tf.float32),\n",
    "        'y': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    }\n",
    "    items_to_handlers = {\n",
    "        'x': slim.tfexample_decoder.Tensor('x'),\n",
    "        'y': slim.tfexample_decoder.Tensor('y'),\n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=tf.TFRecordReader,\n",
    "        decoder=decoder,\n",
    "        num_samples=_SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        labels_to_names=None\n",
    "    )\n",
    "\n",
    "def load_batch(dataset, batch_size=32):\n",
    "    \"\"\"Load data batch\n",
    "    \"\"\"\n",
    "    provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, shuffle=False)\n",
    "    x, y = provider.get(['x', 'y'])\n",
    "    return tf.train.batch(\n",
    "        [x, y],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=1,\n",
    "        capacity=100\n",
    "    )\n",
    "\n",
    "def build_model(x):\n",
    "    \"\"\"Build the neural network\n",
    "    \"\"\"\n",
    "    net = slim.fully_connected(x, 4, activation_fn=tf.nn.relu, scope='fc1')\n",
    "    logits = slim.fully_connected(net, 2, activation_fn=None, scope='fc2')\n",
    "    return logits\n",
    "\n",
    "\n",
    "_FILE_PATTERN = 'moons_%s.tfrecord'\n",
    "train_filename = _FILE_PATTERN % \"train\"\n",
    "test_filename = _FILE_PATTERN % \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "Below is a code snippet that loads the dataset, builds the model, builds the optimizer, and runs training. Take the time to review this code. Run it to train on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dir = \"dynamics/%s\" % datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "# hyperparameters\n",
    "number_of_steps = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "    # load training data\n",
    "    dataset = get_split(\"train\", \".\")\n",
    "    x, y = load_batch(dataset, batch_size=batch_size)\n",
    "\n",
    "    # define model\n",
    "    logits = build_model(x)\n",
    "\n",
    "    # define loss\n",
    "    loss = slim.losses.softmax_cross_entropy(logits, tf.one_hot(y, 2))\n",
    "    tf.scalar_summary(\"loss\", loss)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # define train operation\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer)\n",
    "    \n",
    "    # summary op\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # train the model\n",
    "    slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        log_every_n_steps=number_of_steps / 10,\n",
    "        number_of_steps=number_of_steps,\n",
    "        summary_op=summary_op,\n",
    "        save_summaries_secs=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the loss\n",
    "\n",
    "You can start Tensorboard by running `tensorboard --logdir=dynamics` in the notebook directory and navigating to http://localhost:6006/. Once there, you should be able to click on the \"loss\" section to reveal a plot of the loss over the course of the training run you just did.\n",
    "\n",
    "Below, you will train the same model while changing different hyperparameters.  Each of these runs will show up in Tensorboard, but Tensorboard refreshes slowly. To avoid waiting for new data to show up in Tensorboard, you will have to periodically restart Tensorboard. Each run will be named with a time stamp. If you like, you can clear out all the data if things get too cluttered by simply deleting the `dynamics/` directory that contains the logs.\n",
    "\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "In the loss plot, the loss probably went down, as expected. But notice that at the end of training, it probably looks like the loss was still headed downward. This might mean that we stopped training too early and that we left some improvements on the table.\n",
    "\n",
    "Retrain the model above, but increase the `number_of_steps` to train for longer. Afterwards inspect the loss plot. Is the loss still going down? Find a number of steps that will result in the loss flattening out in the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "The batch size controls how many training examples are processed by the model at each training step and is an important factor in learning.\n",
    "\n",
    "\n",
    "### Exercise 1 - batch sizes\n",
    "\n",
    "Set the number of steps above to 100 and change the batch size hyperparameters to a small number (try 1). What do you observe about the loss plot in this case? You can try other batch sizes as well. Can you explain your observations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "The most important hyperparameter that controls learning is the **learning rate**.\n",
    "\n",
    "It is hard to know what the learning rate should be to begin with for any given problem, so it takes some experimentation for new problems. The learning rate is the basic knob to turn to control how fast learning takes place (how fast the weights change) and the number of iterations controls how long we train. Ideally we want fast learning.\n",
    "\n",
    "\n",
    "### Exercise 2 - Trading off learning rate and epochs\n",
    "\n",
    "By modifying and running the block above while changing the learning rate, we can explore some common behaviors of training.\n",
    "\n",
    "1. Try a large learning rate - can you get the loss plot to look very jagged? What is happening in this case?\n",
    "2. Try an even larger learning rate - can you get the loss to \"explode\" (i.e. become and stay large)\n",
    "3. Try a small learning rate - can you get the loss plot to look like a straight line going down? How fast is learning in this case?\n",
    "4. Setting `number_of_steps = 200` and `batch_size = 100`, can you find a learning rate that makes the loss below 0.3 without increasing the training time?\n",
    "5. If you set the learning rate to 0.001, how many training steps do you need to make the loss drop below 0.5?\n",
    "6. Try to make your model perform better than 85% test accuracy - what parameters did you use? How good can you do? (The code below will load the latest model and evaluate it on the test data - run it to see your latest model's performance.)\n",
    "\n",
    "Hint: try exploring learning rates across orders of magnitude (0.1, 0.01, 0.001...). Explore combinations of learning rate and number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # load validation data\n",
    "    dataset = get_split(\"validation\", \".\")\n",
    "    x, y = load_batch(dataset, batch_size=dataset.num_samples)\n",
    "    \n",
    "    # build model and retrieve predictions\n",
    "    logits = build_model(x)\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    \n",
    "    # compute accuracy\n",
    "    acc_value_op, acc_update_op = slim.metrics.streaming_accuracy(predictions, y)\n",
    "        \n",
    "    # path to model checkpoint\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    print \"Model checkpoint:\", checkpoint_path\n",
    "    \n",
    "    # compute metrics\n",
    "    accuracy = slim.evaluation.evaluate_once(\n",
    "        master='',\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logdir=train_dir,\n",
    "        num_evals=1,\n",
    "        eval_op=acc_update_op,\n",
    "        final_op=acc_value_op\n",
    "    )\n",
    "    \n",
    "    print \"Accuracy =\", accuracy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
